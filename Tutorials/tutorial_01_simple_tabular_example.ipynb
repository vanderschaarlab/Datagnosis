{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datagnosis Tutorial 01 - simple tabular example\n",
    "\n",
    "*If you prefer, this tutorial is also available on [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1PPcjl9jq6E4j3Qz0cZIQbbQTaeK2qH6b/view?usp=sharing)\n",
    "\n",
    "In this tutorial we will see how to use \"hardness characterization method\" plugins to calculate the hardness scores for the data points in a dataset. We will also plot these values and extract some data points based on these scores. For this tutorial we will be using the iris dataset from scikit learn. For a more realistic dataset checkout tutorials 2 and 3!\n",
    "\n",
    "OK, Lets start!\n",
    "\n",
    "First we import our logger from datagnosis and set the logging level at \"INFO\". If something goes wrong and you want to see more detailed logs, you can change the logging level to \"DEBUG\" or, conversely, if you don't want to see any logs you can remove them with log.remove()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datagnosis.logger as log\n",
    "log.add(sink=sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "df = X.copy(deep=True)\n",
    "df['target'] = y\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some pre-processing on the data if you like, such as scaling. \n",
    "\n",
    "The next key step is to then pass the data to the DataHandler object provided by Datagnosis. This is done by passing the features and the labels separately. The features can be a `pandas.DataFrame`, `numpy.ndarray` or `torch.Tensor`. The labels can be `pandas.series`, `numpy.ndarray` or `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datagnosis.plugins.core.datahandler import DataHandler\n",
    "from datagnosis.plugins.core.models.simple_mlp import SimpleMLP\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "X_train = std_scaler.fit_transform(X_train)\n",
    "X_test = std_scaler.transform(X_test)\n",
    "\n",
    "datahander = DataHandler(X_train, y_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define some values which we will pass to the plugin, such as the model that we want to use to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating our model object, which we both want to use downstream, but also we will use to judge the hardness of the data points\n",
    "model = SimpleMLP()\n",
    "\n",
    "# creating our optimizer and loss function objects\n",
    "learning_rate = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `Plugins` object from Datagnosis. Then by calling `list()` on the we can see all the available plugins that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagnosis absolute\n",
    "from datagnosis.plugins import Plugins\n",
    "\n",
    "plugins = Plugins().list()\n",
    "print(plugins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call `get()` to load up a specific plugin from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcm = Plugins().get(\n",
    "    \"aum\",\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    lr=learning_rate,\n",
    "    epochs=10,\n",
    "    num_classes=3,\n",
    "    logging_interval=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to `fit()` the plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hcm.fit(\n",
    "    datahandler=datahander,\n",
    "    use_caches_if_exist=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the plugin has been fit we can access scores. First, lets get a description of the scores then print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hcm.score_description())\n",
    "print(hcm.scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the scores leaves them difficult to digest, so now we will plot them instead. We can plot 1-dimentional scores in two different ways with `plot_type=\"dist\"` or `plot_type=\"scatter\"`. Why not have a look at both types and compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hcm.plot_scores(axis=1, plot_type=\"dist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the `extract_datapoints` method can be used to select data based on the hcm score. Available methods for extract include `\"top_n\"`, `\"threshold\"` and `\"index\"`. Give them all a go!\n",
    "\n",
    "The following cell takes the hardest 10 data points summarises them in a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(f\"Data points that are hard to classify have scores that are: {hcm.hard_direction()}\")\n",
    "hardest_10 = hcm.extract_datapoints(method=\"top_n\", n=10)\n",
    "\n",
    "display(pd.DataFrame(\n",
    "    data={\n",
    "        \"indices\":hardest_10[0][2],\n",
    "        f\"{X.columns[0]}\": hardest_10[0][0].transpose(0,1)[0],\n",
    "        f\"{X.columns[1]}\": hardest_10[0][0].transpose(0,1)[1],\n",
    "        f\"{X.columns[2]}\": hardest_10[0][0].transpose(0,1)[2],\n",
    "        f\"{X.columns[3]}\": hardest_10[0][0].transpose(0,1)[3],\n",
    "        \"labels\": hardest_10[0][1],\n",
    "        \"scores\": hardest_10[1],\n",
    "    }\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc-check",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
